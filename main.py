import os
import json
import warnings
import uvicorn
import numpy as np
from typing import TypedDict, List, Dict, Optional, Any, Literal
from datetime import datetime

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# LangChain & LangGraph
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langgraph.graph import StateGraph, END

# Optimization
from scipy.optimize import minimize

import time # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€
from collections import deque # í†µê³„ ì €ì¥ì„ ìœ„í•´ ì¶”ê°€

# ---------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° LLM ì´ˆê¸°í™”
# ---------------------------------------------------------
load_dotenv(override=True)
warnings.filterwarnings("ignore")

# ë¡œì»¬ Llama ì„œë²„ ì„¤ì •
llm = ChatOpenAI(
    model="local-llama",
    base_url="http://localhost:8090/v1",
    api_key="no-key-needed",
    temperature=0.0,
    timeout=600
)

embeddings = OpenAIEmbeddings(
    model="local-model",
    base_url="http://localhost:8090/v1",
    api_key="no-key-needed"
)

# ---------------------------------------------------------
# 2. RAG ì§€ì‹ ë² ì´ìŠ¤ (Scenario KB -> FAISS)
# ---------------------------------------------------------
SCENARIO_KB = [
    {
        "id": "EXTREME_BEAR_0000",
        "name": "Deleveraging",
        "desc": "ë§ˆì§„ì½œ ë° ë¶€ì±„ ì¶•ì†Œë¡œ ì¸í•œ ê°•ì œ ë§¤ë„ ì¥ì„¸. í•˜ë½ ë³€ë™ì„± ê·¹ëŒ€í™” ë° ìƒê´€ê´€ê³„ ìˆ˜ë ´.",
        "mu": [-0.3385, 0.1038, 0.5128, -0.1338, -0.2469],
        "vol": [0.3599, 0.34, 0.4803, 0.4954, 0.4017],
        "corr": [
            [1.0, 0.7718, -0.1502, 0.0634, 0.6265],
            [0.7718, 1.0, 0.0574, -0.0311, 0.0945],
            [-0.1502, 0.0574, 1.0, 0.7518, -0.53],
            [0.0634, -0.0311, 0.7518, 1.0, -0.10],
            [0.6265, 0.0945, -0.53, -0.10, 1.0]
        ],
    },
    {
        "id": "BULLISH_0001",
        "name": "Goldilocks",
        "desc": "ì €ë¬¼ê°€Â·ì ì • ì„±ì¥ ì† ì´ìƒì ì¸ ìš°ìƒí–¥. ë‚®ì€ ë³€ë™ì„± ë° ì½œ ì˜µì…˜ ìˆ˜ìµì„± ê°œì„ .",
        "mu": [0.1676, -0.0683, -0.2374, 0.0894, 0.1393],
        "vol": [0.1356, 0.1328, 0.1848, 0.1731, 0.1207],
        "corr": [
            [1.0, 0.7992, -0.0815, 0.0967, 0.6250],
            [0.7992, 1.0, 0.0907, -0.0271, 0.0767],
            [-0.0815, 0.0907, 1.0, 0.8390, -0.52],
            [0.0967, -0.0271, 0.8390, 1.0, -0.10],
            [0.6250, 0.0767, -0.52, -0.10, 1.0]
        ],
    },
]


def _build_scenario_vectorstore():
    docs = []
    for s in SCENARIO_KB:
        text = f"ì‹œë‚˜ë¦¬ì˜¤ëª…: {s['name']}\nìƒí™©ì„¤ëª…: {s['desc']}"
        docs.append(Document(page_content=text, metadata=s))
    return FAISS.from_documents(docs, embeddings)


scenario_vs = _build_scenario_vectorstore()


# ---------------------------------------------------------
# 0. ì„±ëŠ¥ í†µê³„ ì €ì¥ì†Œ (In-memory)
# ---------------------------------------------------------
class PerformanceTracker:
    def __init__(self, window_size=100):
        self.history = deque(maxlen=window_size)

    def add_metric(self, data: dict):
        self.history.append(data)

    def get_stats(self):
        if not self.history: return {}
        avg_latency = sum(d['latency'] for d in self.history) / len(self.history)
        avg_score = sum(d['eval_score'] for d in self.history) / len(self.history)
        total_retries = sum(d['retry_count'] for d in self.history)
        return {
            "avg_latency_ms": round(avg_latency, 2),
            "avg_eval_score": round(avg_score, 2),
            "total_requests": len(self.history),
            "retry_rate": round(total_retries / len(self.history), 2)
        }


tracker = PerformanceTracker()


# ---------------------------------------------------------
# 3. ìƒíƒœ(State) ë° ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ì •ì˜
# ---------------------------------------------------------
class QuantState(TypedDict, total=False):
    # Input Data
    question: str

    news_context: str
    is_price_rising: bool
    market_iv: float

    # Process Data
    bull_opinion: str
    bear_opinion: str
    final_consensus: str

    market_trend: str
    risk_score: float
    news_sentiment: str
    divergence_note: str

    # í‰ê°€ & ê°œì„ 
    eval_score: int
    eval_critique: str
    is_sufficient: bool
    retry_count: int

    # ì„±ëŠ¥ ì¸¡ì •ìš©
    start_time: float
    node_timings: Dict[str, float]

    # Final Outputs
    manager_view: str

    rag_context: str

    expected_returns: List[float]
    vol_vector: List[float]
    correlation_matrix: List[List[float]]
    covariance_matrix: List[List[float]]

    optimal_weights: List[float]


class JudgeOutput(BaseModel):
    final_consensus: str = Field(description="ìƒìŠ¹/í•˜ë½ ì˜ê²¬ì„ ì¢…í•©í•œ ìµœì¢… í•©ì˜ë¬¸")
    market_trend: Literal["Bullish", "Bearish", "Volatile", "Neutral"] = Field(description="ì‹œì¥ ë°©í–¥ì„±")
    risk_score: float = Field(ge=1.0, le=10.0, description="ë¦¬ìŠ¤í¬ ì ìˆ˜")
    news_sentiment: Literal["Positive", "Negative", "Neutral"] = Field(description="ë‰´ìŠ¤ ì‹¬ë¦¬")
    divergence_note: str = Field(description="ê´´ë¦¬ ìš”ì•½")


class EvalOutput(BaseModel):
    score: int = Field(description="ë‹µë³€ì˜ ì •í™•ì„± ë° ê´€ë ¨ì„± ì ìˆ˜ (1~10)")
    is_sufficient: bool = Field(description="ì¶”ê°€ ê°œì„  ì—†ì´ ì±„íƒ ê°€ëŠ¥í•œê°€")
    critique: str = Field(description="ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ ë³´ì™„ì´ í•„ìš”í•œ ë¶€ë¶„ì— ëŒ€í•œ í”¼ë“œë°±")


# ---------------------------------------------------------
# 4. ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°
# ---------------------------------------------------------
def measure_time(node_func):
    def wrapper(state: QuantState):
        start = time.perf_counter()
        result = node_func(state)
        end = time.perf_counter()

        # íƒ€ì´ë° ë°ì´í„° ì—…ë°ì´íŠ¸
        timings = state.get("node_timings", {})
        node_name = node_func.__name__
        timings[node_name] = round((end - start) * 1000, 2)  # ms ë‹¨ìœ„

        if result is None: result = {}
        result["node_timings"] = timings
        return result

    return wrapper


# ---------------------------------------------------------
# 4. LangGraph ë…¸ë“œ ì •ì˜
# ---------------------------------------------------------

@measure_time
def debate_agent_node(state: QuantState):
    """í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°•í™”: í˜ë¥´ì†Œë‚˜ ë° ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ê°•ì œ"""
    news = state["news_context"]
    q = state["question"]
    retry_feedback = f"\n[ì´ì „ ì‹œë„ í”¼ë“œë°±]: {state.get('eval_critique', '')}" if state.get('retry_count', 0) > 0 else ""

    system_msg = SystemMessage(content="ë‹¹ì‹ ì€ ì „ë¬¸ í€€íŠ¸ íŠ¸ë ˆì´ë”ì…ë‹ˆë‹¤. ì œê³µëœ ë‰´ìŠ¤ ì§€í‘œì™€ ë§¤í¬ë¡œ ìƒí™©ì„ ì •ë°€ ë¶„ì„í•˜ì„¸ìš”.")

    bull_p = f"ìƒìŠ¹ë¡ ì ì‹œê°ì—ì„œ ë¶„ì„í•˜ì„¸ìš”.{retry_feedback}\në‰´ìŠ¤: {news}\nì§ˆë¬¸: {q}"
    bear_p = f"í•˜ë½ë¡ ì ì‹œê°ì—ì„œ ë¶„ì„í•˜ì„¸ìš”.{retry_feedback}\në‰´ìŠ¤: {news}\nì§ˆë¬¸: {q}"

    bull_op = llm.invoke([system_msg, HumanMessage(content=bull_p)]).content
    bear_op = llm.invoke([system_msg, HumanMessage(content=bear_p)]).content
    return {"bull_opinion": bull_op, "bear_opinion": bear_op, "retry_count": state.get("retry_count", 0)}

@measure_time
def judge_node(state: QuantState):
    """í•©ì˜ë¬¸ ìƒì„± ë° Divergence ì²´í¬"""
    structured_llm = llm.with_structured_output(JudgeOutput)
    prompt = (
        f"CIOë¡œì„œ Bull/Bear ì˜ê²¬ì„ ë¶„ì„í•˜ì—¬ ìµœì¢… í•©ì˜ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"Bull: {state['bull_opinion']}\n"
        f"Bear: {state['bear_opinion']}\n\n"
        f"ì¤‘ìš”: í˜„ì¬ ê°€ê²©ì€ {'ìƒìŠ¹' if state['is_price_rising'] else 'ë¶€ì§„'} ì¤‘ì…ë‹ˆë‹¤. ì´ë¥¼ ë°˜ì˜í•´ [Divergence]ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    )
    res = structured_llm.invoke([HumanMessage(content=prompt)])
    return {
        "final_consensus": res.final_consensus,
        "market_trend": res.market_trend,
        "risk_score": res.risk_score,
        "news_sentiment": res.news_sentiment,
        "divergence_note": res.divergence_note
    }

@measure_time
def evaluator_node(state: QuantState):
    """ ìƒì„±ëœ ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ê´€ë ¨ì„± í‰ê°€"""
    structured_eval = llm.with_structured_output(EvalOutput)

    prompt = (
        "ë‹¹ì‹ ì€ ê¸ˆìœµ ë¦¬í¬íŠ¸ ê°ì‚¬ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ìƒì„±ëœ ê²°ë¡ ì´ ì›ë³¸ ë‰´ìŠ¤ ìƒí™©ê³¼ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë¶€í•©í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n\n"
        f"[ì›ë³¸ ì§ˆë¬¸]: {state['question']}\n"
        f"[ì°¸ê³  ë‰´ìŠ¤]: {state['news_context']}\n"
        f"[ìƒì„±ëœ ê²°ë¡ ]: {state['final_consensus']}\n\n"
        "í‰ê°€ ê¸°ì¤€: 1. ì§ì ‘ì ì¸ ì§ˆë¬¸ ë‹µë³€ ì—¬ë¶€ 2. ìµœì‹  ì§€í‘œ ë°˜ì˜ ì—¬ë¶€ 3. ë…¼ë¦¬ì  ëª¨ìˆœì„±\n"
        "8ì  ë¯¸ë§Œì´ê±°ë‚˜ ê°œì„ ì´ í•„ìš”í•˜ë©´ is_sufficientë¥¼ falseë¡œ ì„¤ì •í•˜ê³  êµ¬ì²´ì ì¸ ë¹„íŒ(critique)ì„ ì‘ì„±í•˜ì„¸ìš”."
    )

    eval_res = structured_eval.invoke([HumanMessage(content=prompt)])

    # ìµœëŒ€ 2íšŒ ì¬ì‹œë„ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    if state.get("retry_count", 0) >= 1:
        eval_res.is_sufficient = True

    return {
        "eval_score": eval_res.score,
        "is_sufficient": eval_res.is_sufficient,
        "eval_critique": eval_res.critique,
        "retry_count": state.get("retry_count", 0) + 1
    }

@measure_time
def scenario_rag_node(state: QuantState):
    """ ê²€ìƒ‰ ì „ëµ ì¡°ì • - í•©ì˜ë¬¸ ê¸°ë°˜ ê³ ë„í™” ê²€ìƒ‰"""
    # ë‹¨ìˆœ ì§ˆë¬¸ ê²€ìƒ‰ì´ ì•„ë‹Œ, í‰ê°€ë¥¼ í†µê³¼í•œ 'í•©ì˜ë¬¸'ì„ ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ì‚¬ìš©

    query = f"{state['market_trend']} {state['final_consensus'][:200]}"
    top = scenario_vs.similarity_search(query, k=1)

    if not top:
        return {"rag_context": "[ì°¸ì¡° ê³¼ê±° ì‹œë‚˜ë¦¬ì˜¤ ì—†ìŒ]"}

    md = top[0].metadata

    anchor_info = (
        f"\n[RAG Anchor: {md['name']}]\n- ì„¤ëª…: {md['desc']}\n"



        f"- ê¸°ì¤€ mu: {md['mu']}\n- ê¸°ì¤€ vol: {md['vol']}\n- ê¸°ì¤€ corr: {md['corr']}"

    )

    # ìµœì¢… manager_view ìƒì„±
    price_action_text = "ìƒìŠ¹" if state["is_price_rising"] else "ë¶€ì§„/í•˜ë½"
    view = "ğŸ“Œ [AI Debate Market View]\n"
    view += f"- News Sentiment: {state['news_sentiment']}\n- Price Action: {price_action_text}\n\n"
    view += f"âš–ï¸ [Judge Consensus]\n{state['final_consensus']}\n\n"
    view += f"[Divergence]\n- {state['divergence_note']}\n"
    view += f"[RAG Context]\n{anchor_info}\n"

    return {
        "rag_anchor_id": md.get("id"),

        "anchor_mu": md.get("mu"),
        "anchor_vol": md.get("vol"),
        "anchor_corr": md.get("corr"),
        "rag_context": anchor_info,
        "manager_view": view
    }

@measure_time
def quant_engine_node(state: QuantState):
    """LLMì„ ì´ìš©í•œ ì •ëŸ‰ì  íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •(Fine-tuning)"""

    prompt = (
        "SYSTEM: You are a quantitative risk management engine.\n"



        "ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [í˜„ì¬ ìƒí™©]ê³¼ RAGë¡œ ì¶”ì¶œëœ [ì°¸ê³  Anchor]ì„ ê²°í•© ë¶„ì„í•˜ì—¬ "
        "5ê°œ ìì‚°ì˜ ê¸°ëŒ€ìˆ˜ìµë¥ (mu), ë³€ë™ì„±(vol), ìƒê´€ê³„ìˆ˜(corr)ë¥¼ JSONìœ¼ë¡œ ì¶”ì •í•˜ì„¸ìš”.\n\n"

        f"í˜„ì¬ ìƒí™©: {state['final_consensus']}\n"
        f"IV: {state['market_iv']}%\n"
        f"ì°¸ê³  Anchor: {state['rag_context']}\n\n"
        "ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."

    )

    try:
        raw_response = llm.invoke([HumanMessage(content=prompt)]).content
        # JSON íŒŒì‹± ë¡œì§ (ê°„ì†Œí™”)
        start = raw_response.find("{")
        end = raw_response.rfind("}") + 1
        data = json.loads(raw_response[start:end])

        mu = data.get("mu", [0.01] * 5)
        vol = data.get("vol", [0.2] * 5)
        corr = np.array(data.get("corr", np.eye(5).tolist()))
        sigma = np.outer(vol, vol) * corr

        return {
            "expected_returns": mu,
            "vol_vector": vol,
            "correlation_matrix": corr.tolist(),
            "covariance_matrix": sigma.tolist()
        }

    except Exception as e:
        print(f"âš ï¸ Quant Engine Error: {e}")

        return {"expected_returns": [0.01] * 5}

@measure_time
def slsqp_optimizer_node(state: QuantState):
    """ìˆ˜í•™ì  ìµœì í™”ë¡œ ìµœì¢… ë¹„ì¤‘ ì‚°ì¶œ"""
    mu = np.array(state.get("expected_returns", [0.01] * 5))

    def obj(w): return -np.dot(w, mu)

    cons = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0})
    bounds = [(0, 0.45)] * 5
    res = minimize(obj, [0.2] * 5, method='SLSQP', bounds=bounds, constraints=cons)
    return {"optimal_weights": res.x.tolist()}


# ---------------------------------------------------------
# 5. ì›Œí¬í”Œë¡œìš° êµ¬ì„± (í‰ê°€ ë° ì¡°ê±´ë¶€ ë£¨í”„)
# ---------------------------------------------------------
def decide_refinement(state: QuantState):
    """í‰ê°€ ì ìˆ˜ì— ë”°ë¼ ì¬ìˆ˜í–‰ ì—¬ë¶€ ê²°ì •"""
    if state.get("is_sufficient"):
        return "approved"
    return "refine"


workflow = StateGraph(QuantState)

workflow.add_node("Debate", debate_agent_node)
workflow.add_node("Judge", judge_node)
workflow.add_node("Evaluate", evaluator_node)  # í‰ê°€ ë…¸ë“œ

workflow.add_node("ScenarioRAG", scenario_rag_node)
workflow.add_node("QuantEngine", quant_engine_node)

workflow.add_node("Optimizer", slsqp_optimizer_node)

workflow.set_entry_point("Debate")
workflow.add_edge("Debate", "Judge")
workflow.add_edge("Judge", "Evaluate")

# ì¡°ê±´ë¶€ ì—£ì§€: ë¶ˆì¶©ë¶„í•˜ë©´ ë‹¤ì‹œ í† ë¡ ìœ¼ë¡œ, ì¶©ë¶„í•˜ë©´ RAGë¡œ ì§„í–‰
workflow.add_conditional_edges(
    "Evaluate",
    decide_refinement,
    {
        "refine": "Debate",
        "approved": "ScenarioRAG"
    }
)

workflow.add_edge("ScenarioRAG", "QuantEngine")
workflow.add_edge("QuantEngine", "Optimizer")
workflow.add_edge("Optimizer", END)

workflow_app = workflow.compile()

# ---------------------------------------------------------
# 6. FastAPI ì„œë²„ êµ¬ì¶•
# ---------------------------------------------------------
api_app = FastAPI(title="Test, Self-Improving Financial RAG API")


class AnalyzeRequest(BaseModel):
    question: str


@api_app.post("/analyze")
async def analyze_market(request: AnalyzeRequest):
    overall_start = time.perf_counter()

    # ê°€ìƒì˜ ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ë° ì§€í‘œ ì¤€ë¹„ (ì „ì²˜ë¦¬)

    test_news = "2026-02-03 | ì›Œì‹œ ì—°ì¤€ ì˜ì¥ í›„ë³´ ì§€ëª…ì— ë”°ë¥¸ ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€ê° í˜¼ì¡° ë° í™˜ìœ¨ í•˜ë½"

    try:
        initial_state = {
            "question": request.question,
            "news_context": test_news,
            "is_price_rising": True,
            "market_iv": 12.0,
            "retry_count": 0,
            "node_timings": {}
        }

        result = workflow_app.invoke(initial_state)

        overall_end = time.perf_counter()
        total_latency = (overall_end - overall_start) * 1000

        # ë©”íŠ¸ë¦­ ì €ì¥
        tracker.add_metric({
            "latency": total_latency,
            "eval_score": result.get("eval_score", 0),
            "retry_count": result.get("retry_count", 0)
        })


        return {
            "status": "success",
            "performance": {
                "total_latency_ms": round(total_latency, 2),
                "node_breakdown": result.get("node_timings")
            },
            "evaluation": {
                "final_score": result.get("eval_score"),
                "total_attempts": result.get("retry_count"),
                "critique": result.get("eval_critique")
            },
            "manager_view": result.get("manager_view"),
            "quant_params": {
                "expected_returns": result.get("expected_returns"),

                "risk_score": result.get("risk_score")
            },
            "portfolio": {

                "weights": [round(w, 4) for w in (result.get("optimal_weights") or [])]
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@api_app.get("/metrics")
async def get_system_metrics():
    """ì‹œìŠ¤í…œ ì „ì²´ ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜."""
    stats = tracker.get_stats()
    if not stats:
        return {"message": "No data collected yet."}
    return stats


if __name__ == "__main__":
    uvicorn.run(api_app, host="0.0.0.0", port=8088)
